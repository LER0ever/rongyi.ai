[{"authors":["rongyi"],"categories":null,"content":"欢迎来到我的学术网站，这里会更新一些我在科研方向的项目进展。与此同时，欢迎访问我的个人网站 https://rongyi.io 以及我的博客 https://rongyi.blog。\n 今年九月我将进入 加州大学圣地亚哥分校攻读计算机科学\u0026amp;工程专业硕士项目。\n 更新: 由于新冠疫情以及签证延迟，目前我在考虑接下来的一年(2020-2021)再找一份实习/全职工作。\n 去年暑假开始的一年，我在阿里巴巴机器学习平台PAI做研究型实习生。\n我的本科就读于威斯康辛大学麦迪逊分校，计算机科学和数学双专业。2019年5月，我于大三下学期毕业，两个专业GPA均高于3.91。\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"zh","lastmod":1596529992,"objectID":"a895eb28463be61a7aa6a625c8ee875a","permalink":"https://rongyi.ai/zh/author/%E8%8D%A3%E6%87%BF/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/zh/author/%E8%8D%A3%E6%87%BF/","section":"authors","summary":"欢迎来到我的学术网站，这里会更新一些我在科研方向的项目进展。与此同时，欢迎访问我的个人网站 https://rongyi.io 以及我的博客 https://rongyi.blog。\n 今年九月我将进入 加州大学圣地亚哥分校攻读计算机科学\u0026amp;工程专业硕士项目。\n 更新: 由于新冠疫情以及签证延迟，目前我在考虑接下来的一年(2020-2021)再找一份实习/全职工作。\n 去年暑假开始的一年，我在阿里巴巴机器学习平台PAI做研究型实习生。\n我的本科就读于威斯康辛大学麦迪逊分校，计算机科学和数学双专业。2019年5月，我于大三下学期毕业，两个专业GPA均高于3.91。","tags":null,"title":"荣懿","type":"authors"},{"authors":["Siyu Wang","荣懿","Shiqing Fan","Zhen Zheng","LanSong Diao","Guoping Long","Jun Yang","Xiaoyong Liu","Wei Lin"],"categories":["Distributed Training","Reinforcement Learning"],"content":"","date":1594194779,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1596529992,"objectID":"1f9f6672cbf7545321841735f466ae97","permalink":"https://rongyi.ai/zh/publication/automap/","publishdate":"2020-08-04T15:52:59+08:00","relpermalink":"/zh/publication/automap/","section":"publication","summary":"The last decade has witnessed growth in the computational requirements for training deep neural networks. Current approaches (e.g., data/model parallelism, pipeline parallelism) parallelize training tasks onto multiple devices. However, these approaches always rely on specific deep learning frameworks and requires elaborate manual design, which make it difficult to maintain and share between different type of models. In this paper, we propose Auto-MAP, a framework for exploring distributed execution plans for DNN workloads, which can automatically discovering fast parallelization strategies through reinforcement learning on IR level of deep learning models. Efficient exploration remains a major challenge for reinforcement learning. We leverage DQN with task-specific pruning strategies to help efficiently explore the search space including optimized strategies. Our evaluation shows that Auto-MAP can find the optimal solution in two hours, while achieving better throughput on several NLP and convolution models.","tags":["DQN","Data Parallelism","Sharding","Pipeline Parallelism","Deep Reinforcement Learning"],"title":"Auto-MAP: A DQN Framework for Exploring Distributed Execution Plans for DNN Workloads","type":"publication"},{"authors":["Shiqing Fan","荣懿","Chen Meng","Zongyan Cao","Siyu Wang","Zhen Zheng","Chuan Wu","Guoping Long","Jun Yang","Lixue Xia","Lansong Diao","Xiaoyong Liu","Wei Lin"],"categories":["Distributed Training"],"content":"","date":1587116639,"expirydate":-62135596800,"kind":"page","lang":"zh","lastmod":1596529992,"objectID":"05daecc5d055500ea8dfe66351360d19","permalink":"https://rongyi.ai/zh/publication/dapple/","publishdate":"2020-04-17T17:43:59+08:00","relpermalink":"/zh/publication/dapple/","section":"publication","summary":"We propose DAPPLE, a synchronous training framework which combines data parallelism and pipeline parallelism for large DNN models. It features a novel parallelization strategy planner to solve the partition and placement problems, and explores the optimal hybrid strategy of data and pipeline parallelism. We also propose a new runtime scheduling algorithm...","tags":["Data Parallelism","Pipeline Parallelism","Hybrid Parallelism","Dynamic Programming"],"title":"DAPPLE: A Pipelined Data Parallel Approach for Training Large Models","type":"publication"}]