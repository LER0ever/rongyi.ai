[{"authors":["rongyi"],"categories":null,"content":"Welcome to my Academic site, where I share updates on my research projects.\n _2023 \u0026 beyond_ | **I am actively seeking PhD advisors for 2023**-- 2021 - now | I am a Master\u0026rsquo;s student at University of California, San Diego majoring in Computer Science \u0026amp; Engineering.\n2020 - 2021 | Due to COVID-19 hardships and visa delays, I deferred my master\u0026rsquo;s to Fall 2021, and worked as a research intern at ByteDance AI Lab. I worked on several research projects on large-scale Machine Learning systems, DNN Compilers, and Automatic Parallelization algorithms.\n2019 - 2020 | In the 2019 school year, I was working as a year-round Research Intern at Alibaba Platform of AI (PAI), where I was lucky enough to work with a fantastic team on solving problems in the systems side of Machine Learning. My main focus was the automatic planning of Hybrid Parallelism strategy for Deep Learning.\n-- 2016 - 2019 | I obtained my Bacholar\u0026rsquo;s degree at University of Wisconsin - Madison. In my undergraduate years, I double-majored in Computer Science and Mathematics there, and maintained both major GPAs above 3.91. I graduated with distinction in my junior year, 2019.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1656102460,"objectID":"a895eb28463be61a7aa6a625c8ee875a","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Welcome to my Academic site, where I share updates on my research projects.\n _2023 \u0026 beyond_ | **I am actively seeking PhD advisors for 2023**-- 2021 - now | I am a Master\u0026rsquo;s student at University of California, San Diego majoring in Computer Science \u0026amp; Engineering.","tags":null,"title":"Yi Rong","type":"authors"},{"authors":["Yi Rong"],"categories":["NLP","Machine Learning"],"content":"In this report, we discuss the various ways of building probabilistic language models, specifically N-grams. Using the given corpora from three different domains, we first evaluate the reference Unigram implementation provided in the starter code in Section 2. Then we propose our Trigram approach in Section 3, with the implementation details explained inSection 3.1. We show that our approach outperforms the Unigram baseline in almost every performance metric, in Section 3.2 and 3.3. Finally we explore the possibility of adapting our language model from one corpus to another, and demonstrated significant improvement in perplexity in Section 4. Finally we conclude our report in Section 5.\nFor the Content-aware Language Model experiments, we implemented a generic N-gram model with two optimizations:\n Interpolation: The probability estimates from N-gram down to unigram are mixed and weighted (3.1.1), and the the weights λs are dynamically tuned using EM Algorithm (3.1.2). Smoothing: Instead of using Laplace Smoothing (add-1), we added hyper-parameter k and implemented Add-k Smoothing, with k being tuned on a dev set (3.1.3). Low frequency cut-off: Taking a parameter min_freq, we remove all the rare item in vocab and treat them as “UNK”  ","date":1651045979,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651725050,"objectID":"3ddc9ca93a0c2cad65f15dd4594f2438","permalink":"https://rongyi.ai/report/cse256-a3-report/","publishdate":"2022-05-04T15:52:59+08:00","relpermalink":"/report/cse256-a3-report/","section":"report","summary":"In this report, we discuss the various ways of building probabilistic language models, specifically N-grams. Using the given corpora from three different domains, we first evaluate the reference Unigram implementation provided in the starter code in Section 2.","tags":["N-Gram","Unigram","Bigram","Trigram","EM Algorithm"],"title":"CSE256 Assignment 3: Language Modeling","type":"report"},{"authors":["Yi Rong"],"categories":["NLP","Machine Learning"],"content":"In this report, we discuss the various ways of data pre-processing and feature engineering for a text classification task. We first start by giving an overview of the classification task, the model used, and the given baseline implementation in Section 2. Then we iterate on top that version guided by the project documentation to use TF-IDF for token weighting to achieve better accuracy, detailed in Section 3. Finally we present our various approaches for feature extraction and pre-processing, such as BPE [2] and Word2Vec [1] in Section 4. We will discuss the accuracy and other performance metrics of the above approaches in Section 5, will conclude the paper in Section 6.\n","date":1649663579,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651725050,"objectID":"01bdacb1cc6200ef75a56b989ec8fe8c","permalink":"https://rongyi.ai/report/cse256-a1-report/","publishdate":"2022-05-04T15:52:59+08:00","relpermalink":"/report/cse256-a1-report/","section":"report","summary":"In this report, we discuss the various ways of data pre-processing and feature engineering for a text classification task. We first start by giving an overview of the classification task, the model used, and the given baseline implementation in Section 2.","tags":["Text Classification","BPE","Word2Vec","Transformer"],"title":"CSE256 Assignment 1: Text Classification","type":"report"},{"authors":["Huiwen Lu","Kanlin Wang","Yi Rong","Sihan Liu"],"categories":["Statistics","Machine Learning"],"content":"In traditional linear regression, we try to recover a hidden model parameter $\\vec w*$ with samples $(\\vec x, y)$ of the form $y = \\vec {w}^{*T} \\vec x + \\epsilon$, where $\\epsilon$ is sampled from some noise distribution. Classical results show that $\\vec w*$ can be recovered within the $\\ell_2$-reconstruction error $O(\\sqrt{k/n})$, where $n$ is the number of truncated/observable samples, and $k$ is the dimension of $\\vec w^*$. However, this kind of classic technique does not apply to partially observable data, namely, the truncated setting. Analysis from truncated samples is one of the biggest challenge in today\u0026rsquo;s life cause truncation always happened whenever samples not in the bound are not observed. This kind of case is very common in biological search, social science, business and economics field either due to the limitation of data collecting device or inherit flaws of sampling processes. If we ignore the truncation, as shown in various experiments, the regression result shows very limited generalization property under regions where data is missing. Recently, a series of work (See) has lead to theoretically sound truncated linear regression with optimal sample complexity since the challenge was introduced in 1958. While these are all polynomial-time algorithms, their run-time is far-from being practical due to some complicated projection steps that rely heavily on the ellipsoid methods. On the other hand, these works often assume that the data truncation is arbitrary and only oracle access to the truncation set is provided while, in practice, the censoring setting, where the truncation set is convex, is more ubiquitous.\nOur Contributions. In this work, we give an efficient truncated linear-regression algorithm tailored for the censoring setting. The run-time of the algorithm is in the worst case bounded by $O(n^3)$, where $n$ is the input data-size, and is on-average better when the data follows certain structured distributions, showing that truncated linear regression can be practical.\n","date":1647330779,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651725050,"objectID":"86c032843a8b71c42e79de366889c750","permalink":"https://rongyi.ai/report/cse203b-report/","publishdate":"2020-08-04T15:52:59+08:00","relpermalink":"/report/cse203b-report/","section":"report","summary":"In traditional linear regression, we try to recover a hidden model parameter $\\vec w*$ with samples $(\\vec x, y)$ of the form $y = \\vec {w}^{*T} \\vec x + \\epsilon$, where $\\epsilon$ is sampled from some noise distribution.","tags":["DQN","Data Parallelism","Theory","Regression","Truncated Linear Regression"],"title":"CSE203B: Linear Regression under Interval Truncation","type":"report"},{"authors":["Yi Rong"],"categories":["Statistics","Machine Learning"],"content":"In this report, we discuss our attempt to use a better strategy to pick the direction of coordinate descent at each step rather than random selection. At each iteration, we pick the gradient direction with the largest absolute value, and shows that our method outperforms the random coordinate descent in terms of convergence speed.\n","date":1645516379,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651725050,"objectID":"ff672056eb11c132efb0faa7e036fef6","permalink":"https://rongyi.ai/report/cse251a-p2-report/","publishdate":"2022-05-04T15:52:59+08:00","relpermalink":"/report/cse251a-p2-report/","section":"report","summary":"In this report, we discuss our attempt to use a better strategy to pick the direction of coordinate descent at each step rather than random selection. At each iteration, we pick the gradient direction with the largest absolute value, and shows that our method outperforms the random coordinate descent in terms of convergence speed.","tags":["Coordinate Descent","Logistic Regression","SGD","Regression"],"title":"CSE251A Project 2: Coordinate Descent for Logistic Regression","type":"report"},{"authors":["Yi Rong"],"categories":["Statistics","Machine Learning"],"content":"In the report, we discuss our attempt to choose a better prototype for nearest neighbor other than random selection.We present a KMeans based prototype selection method that clearly outperforms the naive random selection in all our experiments.\n","date":1643442779,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651725050,"objectID":"e8902f05dd1f734f85b52e99dc6e08a2","permalink":"https://rongyi.ai/report/cse251a-p1-report/","publishdate":"2022-05-04T15:52:59+08:00","relpermalink":"/report/cse251a-p1-report/","section":"report","summary":"In the report, we discuss our attempt to choose a better prototype for nearest neighbor other than random selection.We present a KMeans based prototype selection method that clearly outperforms the naive random selection in all our experiments.","tags":["KNN ","K-Means","Nearest Neighbor"],"title":"CSE251A Project 1: K-Means Clustering based Prototype Selection for Nearest Neighbor","type":"report"},{"authors":null,"categories":null,"content":"Ways to reach me I have a public list of Email addresses / phone numbers / social media here at rongyi.xyz/contact, but I\u0026rsquo;d prefer to communicate over Email.\n   Special Notice for Email Note that the rongyi.* Email system has a strict global blacklist to combat spam \u0026amp; ads, which would block the following common email providers or patterns:\n Tencent / QQ Email: *@qq.com, *@*.qq.com Netease / 163: *@163.com, *@126.com Self-hosted SMTP/IMAP without SPF, DKIM, DMARC  ","date":1640390400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640554825,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"https://rongyi.ai/contact/","publishdate":"2021-12-25T00:00:00Z","relpermalink":"/contact/","section":"","summary":"Ways to reach me I have a public list of Email addresses / phone numbers / social media here at rongyi.xyz/contact, but I\u0026rsquo;d prefer to communicate over Email.\n   Special Notice for Email Note that the rongyi.","tags":null,"title":"Contact Me","type":"page"},{"authors":["Siyu Wang","Yi Rong","Shiqing Fan","Zhen Zheng","LanSong Diao","Guoping Long","Jun Yang","Xiaoyong Liu","Wei Lin"],"categories":["Distributed Training","Reinforcement Learning"],"content":"","date":1594194779,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625243726,"objectID":"1f9f6672cbf7545321841735f466ae97","permalink":"https://rongyi.ai/publication/automap/","publishdate":"2020-08-04T15:52:59+08:00","relpermalink":"/publication/automap/","section":"publication","summary":"The last decade has witnessed growth in the computational requirements for training deep neural networks. Current approaches (e.g., data/model parallelism, pipeline parallelism) parallelize training tasks onto multiple devices. However, these approaches always rely on specific deep learning frameworks and requires elaborate manual design, which make it difficult to maintain and share between different type of models. In this paper, we propose Auto-MAP, a framework for exploring distributed execution plans for DNN workloads, which can automatically discovering fast parallelization strategies through reinforcement learning on IR level of deep learning models. Efficient exploration remains a major challenge for reinforcement learning. We leverage DQN with task-specific pruning strategies to help efficiently explore the search space including optimized strategies. Our evaluation shows that Auto-MAP can find the optimal solution in two hours, while achieving better throughput on several NLP and convolution models.","tags":["DQN","Data Parallelism","Sharding","Pipeline Parallelism","Deep Reinforcement Learning"],"title":"Auto-MAP: A DQN Framework for Exploring Distributed Execution Plans for DNN Workloads","type":"publication"},{"authors":["Shiqing Fan","Yi Rong","Chen Meng","Zongyan Cao","Siyu Wang","Zhen Zheng","Chuan Wu","Guoping Long","Jun Yang","Lixue Xia","Lansong Diao","Xiaoyong Liu","Wei Lin"],"categories":["Distributed Training"],"content":"","date":1587116639,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625243726,"objectID":"05daecc5d055500ea8dfe66351360d19","permalink":"https://rongyi.ai/publication/dapple/","publishdate":"2020-04-17T17:43:59+08:00","relpermalink":"/publication/dapple/","section":"publication","summary":"We propose DAPPLE, a synchronous training framework which combines data parallelism and pipeline parallelism for large DNN models. It features a novel parallelization strategy planner to solve the partition and placement problems, and explores the optimal hybrid strategy of data and pipeline parallelism. We also propose a new runtime scheduling algorithm...","tags":["Data Parallelism","Pipeline Parallelism","Hybrid Parallelism","Dynamic Programming"],"title":"DAPPLE: A Pipelined Data Parallel Approach for Training Large Models","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1616488679,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://rongyi.ai/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]