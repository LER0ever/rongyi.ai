[{"authors":["rongyi"],"categories":null,"content":"Welcome to my Academic site, where I share updates on my research projects.\n University of California, San Diego in Fall 2020 as a master's student majoring in Computer Science \u0026 Engineering. --  Update: Due to COVID-19 hardships and visa delays, I\u0026rsquo;m deferring my master\u0026rsquo;s to Fall 2021, and working at ByteDance AI Lab for this year.\n In the 2019 school year (2019/07 - 2020/07), I was working as a year-round Research Intern at Alibaba Platform of AI (PAI), where I was lucky enough to work with a fantastic team on solving problems in the systems side of Machine Learning. My main focus was the automatic planning of Hybrid Parallelism strategy for Deep Learning.\n-- I obtained my Bacholar\u0026rsquo;s degree at University of Wisconsin - Madison. In my undergraduate years, I double-majored in Computer Science and Mathematics there, and maintained both major GPAs above 3.91. I graduated with distinction in my junior year, 2019.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1604829966,"objectID":"a895eb28463be61a7aa6a625c8ee875a","permalink":"https://rongyi.ai/author/yi-rong/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yi-rong/","section":"authors","summary":"Welcome to my Academic site, where I share updates on my research projects.\n University of California, San Diego in Fall 2020 as a master's student majoring in Computer Science \u0026 Engineering.","tags":null,"title":"Yi Rong","type":"authors"},{"authors":["Siyu Wang","Yi Rong","Shiqing Fan","Zhen Zheng","LanSong Diao","Guoping Long","Jun Yang","Xiaoyong Liu","Wei Lin"],"categories":["Distributed Training","Reinforcement Learning"],"content":"","date":1594194779,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596528640,"objectID":"1f9f6672cbf7545321841735f466ae97","permalink":"https://rongyi.ai/publication/automap/","publishdate":"2020-08-04T15:52:59+08:00","relpermalink":"/publication/automap/","section":"publication","summary":"The last decade has witnessed growth in the computational requirements for training deep neural networks. Current approaches (e.g., data/model parallelism, pipeline parallelism) parallelize training tasks onto multiple devices. However, these approaches always rely on specific deep learning frameworks and requires elaborate manual design, which make it difficult to maintain and share between different type of models. In this paper, we propose Auto-MAP, a framework for exploring distributed execution plans for DNN workloads, which can automatically discovering fast parallelization strategies through reinforcement learning on IR level of deep learning models. Efficient exploration remains a major challenge for reinforcement learning. We leverage DQN with task-specific pruning strategies to help efficiently explore the search space including optimized strategies. Our evaluation shows that Auto-MAP can find the optimal solution in two hours, while achieving better throughput on several NLP and convolution models.","tags":["DQN","Data Parallelism","Sharding","Pipeline Parallelism","Deep Reinforcement Learning"],"title":"Auto-MAP: A DQN Framework for Exploring Distributed Execution Plans for DNN Workloads","type":"publication"},{"authors":["Shiqing Fan","Yi Rong","Chen Meng","Zongyan Cao","Siyu Wang","Zhen Zheng","Chuan Wu","Guoping Long","Jun Yang","Lixue Xia","Lansong Diao","Xiaoyong Liu","Wei Lin"],"categories":["Distributed Training"],"content":"","date":1587116639,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1596528640,"objectID":"05daecc5d055500ea8dfe66351360d19","permalink":"https://rongyi.ai/publication/dapple/","publishdate":"2020-04-17T17:43:59+08:00","relpermalink":"/publication/dapple/","section":"publication","summary":"We propose DAPPLE, a synchronous training framework which combines data parallelism and pipeline parallelism for large DNN models. It features a novel parallelization strategy planner to solve the partition and placement problems, and explores the optimal hybrid strategy of data and pipeline parallelism. We also propose a new runtime scheduling algorithm...","tags":["Data Parallelism","Pipeline Parallelism","Hybrid Parallelism","Dynamic Programming"],"title":"DAPPLE: A Pipelined Data Parallel Approach for Training Large Models","type":"publication"}]