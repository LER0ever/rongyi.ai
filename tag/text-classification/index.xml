<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Text Classification | L.E.R Academic</title><link>https://rongyi.ai/tag/text-classification/</link><atom:link href="https://rongyi.ai/tag/text-classification/index.xml" rel="self" type="application/rss+xml"/><description>Text Classification</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Â© 2022 Yi Rong</copyright><lastBuildDate>Mon, 11 Apr 2022 15:52:59 +0800</lastBuildDate><image><url>https://rongyi.ai/media/icon_hucf03f274847a1149dd55649cb0f12563_327173_512x512_fill_lanczos_center_3.png</url><title>Text Classification</title><link>https://rongyi.ai/tag/text-classification/</link></image><item><title>CSE256 Assignment 1: Text Classification</title><link>https://rongyi.ai/report/cse256-a1-report/</link><pubDate>Mon, 11 Apr 2022 15:52:59 +0800</pubDate><guid>https://rongyi.ai/report/cse256-a1-report/</guid><description>&lt;p>In this report, we discuss the various ways of data pre-processing and feature engineering for a text classification task. We first start by giving an overview of the classification task, the model used, and the given baseline implementation in Section 2. Then we iterate on top that version guided by the project documentation to use TF-IDF for token weighting to achieve better accuracy, detailed in Section 3. Finally we present our various approaches for feature extraction and pre-processing, such as BPE [2] and Word2Vec [1] in Section 4.
We will discuss the accuracy and other performance metrics of the above approaches in Section 5, will conclude the paper in Section 6.&lt;/p></description></item></channel></rss>