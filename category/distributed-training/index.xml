<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Distributed Training | L.E.R Academic</title><link>https://rongyi.ai/category/distributed-training/</link><atom:link href="https://rongyi.ai/category/distributed-training/index.xml" rel="self" type="application/rss+xml"/><description>Distributed Training</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Â© 2021 Yi Rong</copyright><lastBuildDate>Wed, 08 Jul 2020 15:52:59 +0800</lastBuildDate><image><url>https://rongyi.ai/images/icon_hucf03f274847a1149dd55649cb0f12563_327173_512x512_fill_lanczos_center_2.png</url><title>Distributed Training</title><link>https://rongyi.ai/category/distributed-training/</link></image><item><title>Auto-MAP: A DQN Framework for Exploring Distributed Execution Plans for DNN Workloads</title><link>https://rongyi.ai/publication/automap/</link><pubDate>Wed, 08 Jul 2020 15:52:59 +0800</pubDate><guid>https://rongyi.ai/publication/automap/</guid><description/></item><item><title>DAPPLE: A Pipelined Data Parallel Approach for Training Large Models</title><link>https://rongyi.ai/publication/dapple/</link><pubDate>Fri, 17 Apr 2020 17:43:59 +0800</pubDate><guid>https://rongyi.ai/publication/dapple/</guid><description/></item></channel></rss>